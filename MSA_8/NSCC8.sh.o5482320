5tc1_r had ValueError, probably some of inputs are of wrong dimention. Data thrown away 
training samples 4359 from 323 PDBid, val samples 157 from 157 PDBid
Class 1 has 0.0664 percent, given 0.559143602278 weight
Class 2 has 0.0936 percent, given 0.396657427257 weight
Class 3 has 0.84 percent, given 0.0441989704657 weight
4359 finished intitialising total number of training/val samples
['3jct_6', '5t2a_e', '5lyu_a', '4jf2_a', '2quw_b', '3am1_b', '4qjd_b', '5fq5_a', '4v4i_0', '4uje_av', '5m0h_a', '5x2h_b', '5l3p_y', '4v6t_av', '2r8s_r', '4pdb_i', '4qk8_a', '2zzm_b', '4rzd_a', '3j7r_s6', '2z75_b', '3izy_n', '3add_c', '3j0l_1', '3kfu_k', '1ehz_a', '3gx5_a', '5t7v_d', '3ivn_a', '4v6u_b3', '4v5z_ah', '4ug0_s6', '5lzs_ii', '2vaz_a', '4wj4_b', '2oiu_q', '4rum_a', '2zni_c', '5t5h_e', '4wt8_dv', '5aka_7', '3j2c_o', '1pn7_c', '2a64_a', '2dlc_y', '3izz_d', '3izz_b', '3izz_a', '3jcs_8', '3zn8_g', '3jcs_5', '3jcs_3', '1mzp_b', '3eph_e', '3r4f_a', '2noq_b', '2qwy_a', '4mgm_a', '4c4q_n', '2nr0_e', '4v6u_a1', '1t1m_b', '3j46_p', '5ccb_n', '3ivk_c', '1gtn_w', '4v5z_bx', '4k27_u', '5o60_b', '1mvr_a', '3la5_a', '1mvr_d', '4v5z_bi', '5hr6_c', '4p8z_a', '4v5z_bf', '4v5z_be', '1p6v_b', '1n78_c', '2z9q_a', '5e81_1k', '2zjr_y', '3j0o_a', '4n0t_b', '1e8o_e', '3e5c_a', '4wfl_a', '4wzj_v', '4u4r_3', '1fir_a', '1lng_b', '1m5o_b', '3nkb_b', '1h4s_t', '4v2s_q', '3vjr_b', '5ib7_3k', '5b2o_b', '3lwr_d', '1j2b_c', '3p49_a', '4zt0_b', '5o61_bw', '2om7_h', '4lvw_a', '4v6w_bc', '4pkd_v', '4v9i_ay', '5ibb_1l', '3k0j_e', '4jng_l', '5gap_v', '5t5h_d', '5a2t_z', '2om7_g', '4jrc_a', '4v9q_bv', '486d_f', '4aob_a', '5u30_b', '4lx5_a', '4tzx_x', '3jcm_e', '1yls_b', '3icq_d', '5fjc_a', '1vfg_c', '4kr7_x', '2yhm_k', '4adx_9', '1il2_c', '3j16_k', '4v8m_be', '1c0a_b', '3j9w_ax', '4qei_c', '2qus_a', '5e54_a', '3rg5_a', '1qtq_b', '4v8y_cn', '3pu0_r', '5b63_b', '3ep2_b', '2zzn_c', '3gs5_c', '3j7o_7', '5it9_i', '3egz_b', '5czz_b', '4w29_ax', '4wj3_q', '2wwb_d', '3jap_1', '4frg_b', '1zn1_c', '4y4o_1b']
number of weights
1_wc1aa 240
1_wc1ab 3840
1_wc1ac 3840
1_wc1ba 240
1_wc1bb 3840
1_wc1c 240
1_wc1d 240
2_wc1aa 1024
2_wc1ab 3840
2_wc1ac 3840
2_wc1ba 1024
2_wc1bb 3840
2_wc1c 1024
2_wc1d 1024
3_wc1aa 2080
3_wc1ab 15360
3_wc1ac 15360
3_wc1ba 2080
3_wc1bb 16384
3_wc1c 2080
3_wc1d 2080
4_wc1aa 4096
4_wc1ab 15360
4_wc1ac 15360
4_wc1ba 4096
4_wc1bb 16384
4_wc1c 4096
4_wc1d 4096
5_wc1aa 4096
5_wc1ab 15360
5_wc1ac 15360
5_wc1ba 4096
5_wc1bb 50176
5_wc1c 4096
5_wc1d 4096
6_wc1aa 4096
6_wc1ab 15360
6_wc1ac 15360
6_wc1ba 4096
6_wc1bb 50176
6_wc1c 4096
6_wc1d 4096
7_wc1aa 4096
7_wc1ab 15360
7_wc1ac 15360
7_wc1ba 4096
7_wc1bb 50176
7_wc1c 4096
7_wc1d 4096
9_out2 9600
448448 :total parameters
0.59110276935
Epoch: 0001 cost= 0.111775480 0.475744968034
Epoch: 0001 cost= 0.089897729 0.600896230496
0.672248169143
Epoch: 0002 cost= 0.097038247 0.665683539135
Epoch: 0002 cost= 0.086417526 0.679149982123
0.693639014554
Epoch: 0003 cost= 0.095082983 0.720616309058
Epoch: 0003 cost= 0.085730098 0.700645614185
0.699126166138
Epoch: 0004 cost= 0.094541706 0.737662808986
Epoch: 0004 cost= 0.085416406 0.707005769825
0.701638763597
Epoch: 0005 cost= 0.094120391 0.742660038203
Epoch: 0005 cost= 0.085004821 0.709109001585
0.701180451241
Epoch: 0006 cost= 0.093604632 0.742643012216
Epoch: 0006 cost= 0.084749289 0.708849004993
0.700843333552
Epoch: 0007 cost= 0.093140401 0.741132467148
Epoch: 0007 cost= 0.084477179 0.707689552748
0.697583496796
Epoch: 0008 cost= 0.092870973 0.737601452514
Epoch: 0008 cost= 0.084336787 0.706154098973
0.694382902786
Epoch: 0009 cost= 0.092788011 0.730195422163
Epoch: 0009 cost= 0.084412657 0.700668631326
0.690194715141
Epoch: 0010 cost= 0.092790604 0.721383612418
Epoch: 0010 cost= 0.084563076 0.692337225005
0.685674205145
Epoch: 0011 cost= 0.092828892 0.712651661979
Epoch: 0011 cost= 0.084501453 0.687973796312
0.671770250257
Epoch: 0012 cost= 0.092818774 0.706028803746
Epoch: 0012 cost= 0.084689729 0.676971961395
0.666174604427
Epoch: 0013 cost= 0.092826493 0.70066633913
Epoch: 0013 cost= 0.084682427 0.669296575967
0.652501142653
Epoch: 0014 cost= 0.092815876 0.695296627933
Epoch: 0014 cost= 0.084587656 0.655250640869
0.659925309089
Epoch: 0015 cost= 0.092770718 0.689516928759
Epoch: 0015 cost= 0.084531017 0.658109369771
0.655279628431
Epoch: 0016 cost= 0.092725858 0.687294217409
Epoch: 0016 cost= 0.084363118 0.649446165139
0.644388828128
Epoch: 0017 cost= 0.092705622 0.68365191619
Epoch: 0017 cost= 0.084422000 0.640512674623
0.641032328897
Epoch: 0018 cost= 0.092693560 0.679316293946
Epoch: 0018 cost= 0.084354751 0.635696034213
0.639165099647
Epoch: 0019 cost= 0.092709087 0.675100931016
Epoch: 0019 cost= 0.084331132 0.631660804364
0.637340022186
Epoch: 0020 cost= 0.092716500 0.668806429228
Epoch: 0020 cost= 0.084380083 0.627809484385
Optimization Finished!
======================================================================================

			Resource Usage on 2017-09-28 17:03:36.023749:

	JobId: 5482320.wlm01  
	Project: 13000487 
	Exit Status: 1
	NCPUs Requested: 24				NCPUs Used: 24
							CPU Time Used: 06:51:54
	Memory Requested: 110100480kb 				Memory Used: 2818256kb
							Vmem Used: 163784212kb
	Walltime requested: 23:59:00 			Walltime Used: 05:29:15
	
	Execution Nodes Used: (gpu1811:mem=110100480kb:ncpus=24:ngpus=1)
	
 ======================================================================================
